---
- name: Check spark file
  stat: 
    path: "{{ source_dir }}/spark-{{ spark_version }}-bin-hadoop2.7.tgz"
  register: downloaded

- name: Download spark
  get_url: 
    url: 'http://apache.mirror.amaze.com.au/spark/spark-{{ spark_version }}/spark-{{ spark_version }}-bin-hadoop2.7.tgz'
    dest: "{{ source_dir }}"
  when: downloaded.stat.exists == false

- name: Unpack spark source code
  unarchive: 
    src: "{{ source_dir }}/spark-{{ spark_version }}-bin-hadoop2.7.tgz"
    dest: "{{ service_dir }}"
    remote_src: yes

- name: Add SPARK_HOME
  lineinfile:
    dest: /etc/profile
    insertafter: 'EOF'  
    line: 'export SPARK_HOME={{ service_dir }}/spark-{{ spark_version }}-bin-hadoop2.7'
  become: yes

- name: Add path
  lineinfile: 
    dest: /etc/profile
    insertafter: 'EOF'  
    line: 'export PATH="$PATH:{{ service_dir }}/spark-{{ spark_version }}-bin-hadoop2.7/bin"' 
    state: present
  become: yes

- name: Refresh.bashrc
  shell: source /etc/profile
  args: 
    executable: /bin/bash
  become: yes

- name: Generate spark-env.sh for spark
  template: 
    src: spark-env.j2 
    dest: "{{ service_dir }}/spark-{{ spark_version }}-bin-hadoop2.7/conf/spark-env.sh"
    owner: "{{ remote_user }}"
    mode: 0644

- name: Generate hive-site.j2 for spark
  template: 
    src: hive-site.j2 
    dest: "{{ service_dir }}/spark-{{ spark_version }}-bin-hadoop2.7/conf/hive-site.xml"
    owner: "{{ remote_user }}"
    mode: 0644

- name: Generate slaves for spark
  template: 
    src: templates/workers
    dest: "{{ service_dir }}/spark-{{ spark_version }}-bin-hadoop2.7/conf/workers" 
    owner: "{{ remote_user }}"
    mode: 0644

# - hosts: slave1
#   remote_user: "{{ remote_user }}"
#   become: yes
#   tasks:
#   - name: start the spark master
#     command: {{ service_dir }}/spark-{{ spark_version }}-bin-hadoop2.7/sbin/start-master.sh
#     environment:
#       SPARK_LOG_DIR: "{{ service_dir }}/spark/log"
#       SPARK_WORKER_DIR: "{{ service_dir }}/spark/work"
#       SPARK_LOCAL_DIRS: "{{ service_dir }}/spark/data"

# - hosts: slaveGroup
#   remote_user: "{{ remote_user }}"
#   become: yes
#   tasks:
#   - name: start the spark slaves
#     command: {{ service_dir }}/spark-{{ spark_version }}-bin-hadoop2.7/sbin/start-slave.sh spark://{{hostvars['slave1']['ansible_default_ipv4']['address']}}:7077
#     environment:
#       SPARK_LOG_DIR: "{{ service_dir }}/spark/log"
#       SPARK_WORKER_DIR: "{{ service_dir }}/spark/work"
#       SPARK_LOCAL_DIRS: "{{ service_dir }}/spark/data"
